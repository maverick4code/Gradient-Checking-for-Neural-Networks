# Gradient Checking for Neural Networks

This is a small but really important project I worked on as part of my Deep Learning Specialization coursework. The focus here was to **implement gradient checking from scratch**, which is a crucial debugging step in training neural networks.

Gradient checking helps ensure that **backpropagation** is correctly implemented, by comparing it to a **numerical approximation of the gradient**. This saved me from chasing invisible bugs during training!

---

## ğŸ” Whatâ€™s Inside?

This project is broken down into 4 exercises, and here's a quick summary of each one:

### âœ… Exercise 1: `forward_propagation`
- Simple function to calculate:  
  `J(Î¸) = Î¸ Â· x`
- This was the foundation for testing gradient computations.
<!-- Optional visual -->
<!-- ![Equation](./images/j_theta.png) -->

### ğŸ” Exercise 2: `backward_propagation`
- Calculated the derivative of the above equation with respect to Î¸.
- Turned out to be:  
  `dJ/dÎ¸ = x`
<!-- Optional visual -->
<!-- ![Equation](./images/dj_dtheta.png) -->

### ğŸ§ª Exercise 3: `gradient_check`
- Implemented **1D gradient checking**.
- Compared:
  - Gradient from `backward_propagation`
  - Gradient approximation using:  
    `(J(Î¸ + Îµ) - J(Î¸ - Îµ)) / (2Îµ)`
- If the relative difference was small enough (typically `< 1e-7`), it confirmed our gradients were correct.
<!-- Optional visual -->
<!-- ![Equation](./images/gradient_approx.png) -->

### ğŸ§  Exercise 4: `gradient_check_n`
- Scaled things up to **N-dimensional parameters** like in a real neural network.
- Flattened parameters and gradients to vectors, then computed numerical gradient for **each** parameter.
- Compared the entire gradient vector from backprop with its numerical approximation.
- This was where the "real" test began. ğŸ˜…

---

## ğŸ–¼ï¸ Visuals That Helped Me A LOT

Here are a couple of diagrams that really clarified whatâ€™s going on under the hood:

### 1ï¸âƒ£ Backpropagation through a 3-layer neural net

This shows the forward pass through layers (Linear â†’ ReLU â†’ Linear â†’ ReLU â†’ Linear â†’ Sigmoid), followed by cost computation, and finally backward propagation where gradients are calculated for every weight and bias.

![Backpropagation Visual](./gradient-check-backprop.png) <!-- Replace with actual image file path -->

### 2ï¸âƒ£ Parameter Vectorization for Gradient Checking

This one shows how the dictionary of parameters (`W1`, `b1`, ..., `W3`, `b3`) is converted into a single vector using `dictionary_to_vector()` and back using `vector_to_dictionary()`.

This step is necessary to calculate and compare gradients during the numerical check.

![Parameter Vectorization Visual](./parameter-vectorization.png) <!-- Replace with actual image file path -->


## ğŸ§° Tools & Concepts Used

- **NumPy** for numerical operations.
- **Forward and Backward Propagation** logic.
- **Gradient Approximation** using finite differences.
- **Norm-based difference checking**:  
  `difference = ||grad - gradapprox||â‚‚ / (||grad||â‚‚ + ||gradapprox||â‚‚)`
<!-- Optional visual -->
<!-- ![Equation](./images/norm_difference.png) -->

---

## ğŸ¤¯ What I Learned

- Even small mistakes in gradient computation can break a neural network. Gradient checking is like a "sanity check" to prevent that.
- Writing out `dictionary_to_vector` and `vector_to_dictionary` functions helped me deeply understand how neural networks manage their parameters behind the scenes.
- This exercise showed me the **power of math** in making code trustworthy.

---

## ğŸ“Œ Notes

- I avoided using loops wherever possible and stuck to vectorized operations (except where instructed).
- The code is structured for clarity â€” each function is modular and testable.
- Hidden tests passed âœ… ğŸ™Œ

---

## âœï¸ Final Thoughts

This might look like a simple exercise, but it gave me a whole new appreciation for what's happening inside a neural net. I feel much more confident now about debugging deep learning models â€” not just guessing but **mathematically verifying** things.

If youâ€™re diving into machine learning, Iâ€™d highly recommend going through this kind of hands-on gradient check implementation. It really cements the fundamentals. ğŸš€

---

Thanks for reading ğŸ™Œ  
â€” Sagar Shahari ğŸ§ ğŸ’»
